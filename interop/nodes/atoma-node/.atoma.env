HF_CACHE_PATH=~/.cache/huggingface
HF_TOKEN=my_token   # required if you want to access a gated model
ATOMA_NODE_CONFIG_PATH=./config.toml

# ----------------------------------------------------------------------------------
# atoma node configuration

# Postgres Configuration
POSTGRES_DB=atoma
POSTGRES_USER=atoma
POSTGRES_PASSWORD=password
POSTGRES_PORT=5432

# Sui Configuration
SUI_CONFIG_PATH=~/.sui/sui_config

# Atoma Node Service Configuration
ATOMA_SERVICE_PORT=3000

COMPOSE_PROFILES=mistralrs_cpu

# Tracing level
TRACE_LEVEL=info


# ----------------------------------------------------------------------------------
# chat completions server
CHAT_COMPLETIONS_SERVER_PORT=50000
CHAT_COMPLETIONS_MODEL="meta-llama/Llama-3.1-70B-Instruct"
CHAT_COMPLETIONS_MAX_MODEL_LEN=4096 # context length

# vllm backend
# Know more about vllm engine arguments here: https://docs.vllm.ai/en/latest/usage/engine_args.html
VLLM_ENGINE_ARGS=--model ${CHAT_COMPLETIONS_MODEL} --max-model-len ${CHAT_COMPLETIONS_MAX_MODEL_LEN}

# ----------------------------------------------------------------------------------
# embeddings server
EMBEDDINGS_SERVER_PORT=50001
EMBEDDINGS_MODEL=intfloat/multilingual-e5-large-instruct

# tei backend
# Choose one of these based on your GPU architecture:
# CPU:                                  TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
# Volta:                                UNSUPPORTED
# Turing (T4, RTX 2000 series, ...):    TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:turing-1.5
# Ampere 80 (A100, A30):                TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:1.5
# Ampere 86 (A10, A40, ...):            TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:86-1.5
# Ada Lovelace (RTX 4000 series, ...):  TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:89-1.5
# Hopper (H100):                        TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:hopper-1.5
TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:1.5

# ----------------------------------------------------------------------------------
# image generation server
IMAGE_GENERATIONS_SERVER_PORT=50002
IMAGE_GENERATIONS_MODEL=black-forest-labs/FLUX.1-schnell
IMAGE_GENERATIONS_ARCHITECTURE=flux

# mistralrs backend
# Choose one of these based on your GPU architecture:
# CPU:                                  MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cpu-0.3.1
# Volta:                                UNSUPPORTED
# Turing (T4, RTX 2000 series, ...):    MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-75-0.3.1
# Ampere 80 (A100, A30):                MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-80-0.3.1
# Ampere 86 (A10, A40, ...):            MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-86-0.3.1
# Ada Lovelace (RTX 4000 series, ...):  MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-89-0.3.1
# Hopper (H100):                        MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-90-0.3.1
MISTRALRS_IMAGE=ghcr.io/ericlbuehler/mistral.rs:cuda-80-0.3.1

# ----------------------------------------------------------------------------------
# TDX configuration
# Enable TDX by setting ENABLE_TDX=true for confidential compute, otherwise it will be disabled
ENABLE_TDX=false

VLLM_TENSOR_PARALLEL_SIZE=1

# Atoma Service Configuration
ATOMA_SERVICE_CHAT_COMPLETIONS_URLS=http://chat-completions:8000
ATOMA_SERVICE_EMBEDDINGS_URL=http://embeddings:80
ATOMA_SERVICE_IMAGE_GENERATIONS_URL=http://image-generations:80
ATOMA_SERVICE_MODELS=TinyLlama/TinyLlama-1.1B-Chat-v1.0
ATOMA_SERVICE_REVISIONS=main
ATOMA_SERVICE_BIND_ADDRESS=0.0.0.0:3000

# Atoma Sui Configuration
ATOMA_SUI_RPC_NODE=https://fullnode.testnet.sui.io:443
ATOMA_SUI_DB=0x4613c1d42e177c4907fc79a8e7351dcbb9dcb3e7cc7d99c2dee52a66d34c7caa
ATOMA_SUI_PACKAGE_ID=0xb3ff74e28867a56a0f54fad446e51bcb1938923e13918b560c08e65328bde5c3
ATOMA_SUI_USDC_PACKAGE_ID=0xa1ec7fc00a6f40db9693ad1415d0c193ad3906494428cf252621037bd7117e29
ATOMA_SUI_REQUEST_TIMEOUT_SECS=300
ATOMA_SUI_REQUEST_TIMEOUT_NANOS=0
ATOMA_SUI_MAX_CONCURRENT_REQUESTS=10
ATOMA_SUI_LIMIT=100
ATOMA_SUI_NODE_SMALL_IDS=1
ATOMA_SUI_CONFIG_PATH=/Users/horizon/.sui/sui_config/client.yaml
ATOMA_SUI_KEYSTORE_PATH=/Users/horizon/.sui/sui_config/sui.keystore
ATOMA_SUI_CURSOR_PATH=./cursor.toml

# Atoma State Configuration
ATOMA_STATE_DATABASE_URL=postgresql://atoma:atoma@db:5432/atoma

# Atoma Daemon Configuration
ATOMA_DAEMON_BIND_ADDRESS=0.0.0.0:3001
ATOMA_DAEMON_NODE_BADGES=0x00000000002121,1

# Proxy Server Configuration
PROXY_SERVER_ADDRESS=http://localhost:8080
PROXY_SERVER_NODE_PUBLIC_ADDRESS=http://localhost:3002
PROXY_SERVER_COUNTRY=JM

# Atoma P2P Configuration
ATOMA_P2P_HEARTBEAT_INTERVAL_SECS=30
ATOMA_P2P_HEARTBEAT_INTERVAL_NANOS=0
ATOMA_P2P_IDLE_TIMEOUT_SECS=60
ATOMA_P2P_IDLE_TIMEOUT_NANOS=0
ATOMA_P2P_LISTEN_ADDR=/ip4/0.0.0.0/udp/4001/quic-v1
ATOMA_P2P_NODE_SMALL_ID=1
ATOMA_P2P_PUBLIC_URL=https://myurl.com
ATOMA_P2P_COUNTRY=JM
